<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>ZeroSC-SR Paper Demo</title>
  <!-- Tailwind CSS -->
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    .audio-sample {
      margin: 10px 0;
    }
    /* Larger, bolder main section titles */
    .section-title {
      @apply text-3xl font-bold mb-4;
    }
    /* Larger, bolder subsection titles */
    .subsection-title {
      @apply text-2xl font-bold mb-2;
    }
  </style>
</head>

<body class="bg-gray-50">
  <!-- Container -->
  <div class="container mx-auto px-4 py-8 max-w-4xl">
    
    <!-- Header -->
    <header class="text-center mb-12">
      <h1 class="text-4xl font-bold mb-4">
        ZeroSC-SR: Leveraging Data Importance for Efficient Semantic Speech Communication
      </h1>
      <div class="flex flex-wrap justify-center gap-4 mb-2">
        <div class="text-center">
          <p class="font-medium">Zewei Wang</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Guanchong Niu</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Yao Tang</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Man-On Pun</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Tat Ming Lok</p>
        </div>
      </div>
      <p class="text-sm text-gray-600">
        Contact: <span class="font-medium">Zewei Wang</span> (<em>zwwong at link.cuhk.edu.hk</em>)
      </p>
    </header>
    
    <!-- Table of Contents -->
    <nav class="mb-12 bg-white border rounded p-4 shadow">
      <h2 class="text-xl font-bold mb-4">Table of Contents</h2>
      <ul class="list-disc list-inside text-gray-700 ml-4">
        <li><a href="#abstract" class="text-blue-600 hover:underline">Abstract</a></li>
        <li>
          <a href="#proposed-system" class="text-blue-600 hover:underline">Proposed System</a>
          <ul class="list-decimal list-inside ml-6">
            <li>
              <a href="#transmitter" class="text-blue-600 hover:underline">
                Transmitter
              </a>
            </li>
            <li>
              <a href="#sort-match-strategy" class="text-blue-600 hover:underline">
                Sort-Match Strategy for Frequency-Selective Channels
              </a>
            </li>
            <li>
              <a href="#receiver" class="text-blue-600 hover:underline">
                Receiver
              </a>
            </li>
          </ul>
        </li>
        <li>
          <a href="#experiment-results" class="text-blue-600 hover:underline">
            Experiment and Numerical Results
          </a>
          <ul class="list-decimal list-inside ml-6">
            <li><a href="#prompt-length" class="text-blue-600 hover:underline">Prompt Length</a></li>
            <li><a href="#phoneme-acoustic" class="text-blue-600 hover:underline">Phonemes vs. Acoustic Features</a></li>
            <li><a href="#importance-analysis" class="text-blue-600 hover:underline">Importance Analysis of Acoustic Features</a></li>
            <li><a href="#sort-match" class="text-blue-600 hover:underline">Sort-Match Strategy Analysis</a></li>
            <li><a href="#comparison-models" class="text-blue-600 hover:underline">Comparison With Other Models</a></li>
          </ul>
        </li>
        <li><a href="#code" class="text-blue-600 hover:underline">Code</a></li>
        <li><a href="#references" class="text-blue-600 hover:underline">References</a></li>
      </ul>
    </nav>

    <!-- Abstract -->
    <section id="abstract" class="mb-12">
      <h2 class="section-title">Abstract</h2>
      <p class="text-gray-700">
        Advancements in artificial intelligence have enabled the development of more efficient and semantic-aware speech communication systems. 
        This paper introduces an innovative Zero-Shot Semantic Communication System with Speech Reconstruction (ZeroSC-SR), designed to enhance speech transmission and reconstruction in frequency-selective fading channels.
        By efficiently transmitting phoneme IDs and compressed prompt acoustic features, ZeroSC-SR achieves a high compression ratio and reduces transmitted data size, enabling superior performance under low-power and noisy conditions. 
        We investigate the impact of prompt length on recognition accuracy and system performance, identifying a trade-off between performance and energy efficiency and offering practical guidelines for selecting appropriate prompt lengths.
        Analyzing the intermediate transmission data, which includes phoneme IDs and acoustic features at different quantization levels, provides critical insights into their relative importance. These insights directly guide the development of our sort-match strategy, which enhances transmission quality in frequency-selective fading channels by allocating channel resources based on data importance.
        Experimental results show that ZeroSC-SR surpasses conventional speech communication systems, demonstrating its effectiveness in real-world applications that demand reliable and efficient speech communication over challenging channels.
      </p>
    </section>

    <figure class="mb-4">
      <img src="Fig/overview.svg" alt="Proposed System Diagram" class="w-full rounded shadow" />
      <figcaption class="text-center text-sm text-gray-600 mt-2">
        Figure 1: Overview of the ZeroSC-SR system structure.
      </figcaption>
    </figure>

    <!-- Proposed System -->
    <section id="proposed-system" class="mb-12">
      <h2 class="section-title">Proposed System</h2>
      <p class="text-gray-700 mb-4">
        The proposed ZeroSC-SR framework combines semantic (phoneme-level) and acoustic (prompt audio codec) features 
        to enable efficient digital speech transmission under frequency-selective fading channels. 
      </p>

      <!-- (1) Transmitter -->
      <h3 id="transmitter" class="subsection-title">1) Transmitter</h3>
      <p class="text-gray-700 mb-4">
        At the transmitter, speech is split into:
      </p>
      <ul class="list-disc list-inside text-gray-700 mb-4 ml-6">
        <li>
          <strong>Phoneme IDs:</strong> Capturing essential linguistic elements
          via ASR-based phonemization.
        </li>
        <li>
          <strong>Prompt Audio Codec Codes:</strong> Extracted by selecting a short
          speech prompt and compressing it through the EnCodec Model [R1].
        </li>
      </ul>
      <p class="text-gray-700 mb-4">
        These data streams are sorted by importance levels, then converted into binary sequences and protected
        with error-control coding (e.g., LDPC), followed by modulation (e.g., 4-QAM).
      </p>
      <figure class="mb-4">
        <img src="Fig/channel_encoder.svg" alt="Channel Encoder Diagram" class="w-full rounded shadow"/>
        <figcaption class="text-center text-sm text-gray-600 mt-2">
          Figure 2: Channel Encoder Diagram.
        </figcaption>
      </figure>

      <!-- (2) Sort-Match Strategy -->
      <h3 id="sort-match-strategy" class="subsection-title">
        2) Sort-Match Strategy for Frequency-Selective Channels
      </h3>
      <p class="text-gray-700 mb-4">
        To combat frequency-selective fading, ZeroSC-SR ranks data streams by importance levels and matches them to subcarriers sorted by channel
        gain. This ensures critical features are placed on
        stronger subcarriers, maintaining good system performance even under adverse conditions.
      </p>
      <ol class="list-decimal list-inside text-gray-700 mb-4 ml-6">
        <li>
          <strong>Sort Data Streams:</strong> The data streams are sorted in descending order of importance level. 
        </li>
        <li>
          <strong>Sort Subcarriers:</strong> Subcarriers are arranged in descending order
          of channel gain magnitude.
        </li>
        <li>
          <strong>Allocation:</strong> Assign higher-priority data to the stronger subcarriers. 
        </li>
      </ol>

      <!-- (3) Receiver -->
      <h3 id="receiver" class="subsection-title">3) Receiver</h3>
      <p class="text-gray-700 mb-4">
        The receiver reverses the transmitter steps, demodulating and decoding the data
        streams. Phoneme IDs and audio codec codes are recovered and mapped to their
        original order based on stored index information. Finally, the VALL-E
        model [R2] predicts the remaining codec codes, and the EnCodec decoder
        reconstructs the full speech waveform [R1]. This integrated pipeline
        enables a zero-shot capability for unseen speakers, achieving high-quality
        performance in low-power and noisy scenarios.
      </p>
    </section>

    <!-- Experiment and Numerical Results -->
    <section id="experiment-results" class="mb-12">
      <h2 class="section-title">Experiment and Numerical Results</h2>
      <p class="text-gray-700 mb-6">
        The performance of the proposed system is evaluated under varying channel conditions, assuming perfect channel state information. Experiments are conducted using the Libri-light dataset on an NVIDIA RTX A4000. Metrics such as Character Error Rate (CER), Word Error Rate (WER), and speaker similarity provide insights into semantic understanding and speaker fidelity. The table below lists key experimental parameters.
      </p>

      <!-- Experiment Parameters Table (omitted for brevity) -->
      <!-- ...table code remains unchanged... -->

      <!-- Subsequent subsections for experimental results -->
      <!-- ... e.g., Prompt Length, Phonemes vs. Acoustic Features, etc. ... -->

    </section>

    <!-- Code -->
    <section id="code" class="mb-12">
      <h2 class="section-title">Code</h2>
      <p class="text-gray-700 mb-4">
        The complete source code for ZeroSC-SR is publicly available. Feel free to clone the repository and follow the instructions in the README to reproduce our experiments and audio demonstrations.
      </p>
      <a href="https://github.com/WinfredCU/ZeroSC-SR.git" class="text-blue-600 hover:underline">
        GitHub Repository
      </a>
    </section>

    <!-- References -->
    <section id="references" class="mb-12">
      <h2 class="section-title">References</h2>
      <ol class="list-decimal list-inside text-gray-700 ml-6">
        <!-- Keep only the references cited in the text -->
        <li>
          <strong>[R1]</strong> D{\'e}fossez, A., Copet, J., Synnaeve, G., &amp; Adi, Y. (2022).
          "High Fidelity Neural Audio Compression." 
          <em>arXiv preprint arXiv:2210.13438</em>.
        </li>
        <li>
          <strong>[R2]</strong> Wang, C. et al. (2023). 
          "Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers," 
          <em>arXiv preprint arXiv:2301.02111</em>.
        </li>
      </ol>
    </section>

    <!-- Footer -->
    <footer class="mt-12 text-center text-gray-500 text-sm">
      <p class="mb-2">&copy; Wang Zewei</p>
    </footer>
  </div>
</body>
</html>
