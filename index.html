<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ZeroSC-SR Paper Demo</title>
  <!-- Tailwind CSS -->
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    .audio-sample {
      margin: 10px 0;
    }
    .section-title {
      @apply text-2xl font-bold mb-4;
    }
    .subsection-title {
      @apply text-xl font-bold mb-2;
    }
  </style>
</head>

<body class="bg-gray-50">

  <!-- Container -->
  <div class="container mx-auto px-4 py-8 max-w-4xl">
    
    <!-- Header -->
    <header class="text-center mb-12">
      <h1 class="text-4xl font-bold mb-4">ZeroSC-SR: Leveraging Data Importance for Efficient Semantic Speech Communication</h1>
      <div class="flex flex-wrap justify-center gap-4 mb-2">
        <div class="text-center">
          <p class="font-medium">Zewei Wang</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Guanchong Niu</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Yao Tang</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Man-On Pun</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Tat Ming Lok</p>
        </div>
      </div>
      <p class="text-sm text-gray-600">
        Contact: <span class="font-medium">Zewei Wang</span> (<em>zwwong at link.cuhk.edu.hk</em>)
      </p>
    </header>
    
    <!-- Table of Contents (Revised) -->
    <nav class="mb-12 bg-white border rounded p-4 shadow">
      <h2 class="text-xl font-bold mb-4">Table of Contents</h2>
      <ul class="list-disc list-inside text-gray-700 ml-4">
        <li><a href="#abstract" class="text-blue-600 hover:underline">Abstract</a></li>
        <li>
          <a href="#proposed-system" class="text-blue-600 hover:underline">Proposed System</a>
          <ul class="list-decimal list-inside ml-6">
            <li>
              <a href="#transmitter" class="text-blue-600 hover:underline">
                Transmitter
              </a>
            </li>
            <li>
              <a href="#sort-match-strategy" class="text-blue-600 hover:underline">
                Sort-Match Strategy for Frequency-Selective Channels
              </a>
            </li>
            <li>
              <a href="#receiver" class="text-blue-600 hover:underline">
                Receiver
              </a>
            </li>
          </ul>
        </li>
        <li>
          <a href="#experiment-results" class="text-blue-600 hover:underline">
            Experiment and Numerical Results
          </a>
          <ul class="list-decimal list-inside ml-6">
            <li><a href="#prompt-length" class="text-blue-600 hover:underline">Prompt Length</a></li>
            <li><a href="#phoneme-acoustic" class="text-blue-600 hover:underline">Phonemes vs. Acoustic Features</a></li>
            <li><a href="#importance-analysis" class="text-blue-600 hover:underline">Importance Analysis of Acoustic Features</a></li>
            <li><a href="#sort-match" class="text-blue-600 hover:underline">Sort-Match Strategy Analysis</a></li>
            <li><a href="#comparison-models" class="text-blue-600 hover:underline">Comparison With Other Models</a></li>
          </ul>
        </li>
        <li><a href="#code" class="text-blue-600 hover:underline">Code</a></li>
      </ul>
    </nav>

    
    <!-- Abstract -->
    <section id="abstract" class="mb-12">
      <h2 class="section-title">Abstract</h2>
      <p class="text-gray-700">
        Advancements in artificial intelligence have enabled the development of more efficient and semantic-aware speech communication systems. 
        This paper introduces an innovative Zero-Shot Semantic Communication System with Speech Reconstruction (ZeroSC-SR), designed to enhance speech transmission and reconstruction in frequency-selective fading channels.
        By efficiently transmitting phoneme IDs and compressed prompt acoustic features, ZeroSC-SR achieves a high compression ratio and reduces transmitted data size, enabling superior performance under low-power and noisy conditions. 
        We investigate the impact of prompt length on recognition accuracy and system performance, identifying a trade-off between performance and energy efficiency and offering practical guidelines for selecting appropriate prompt lengths.
        Analyzing the intermediate transmission data, which includes phoneme IDs and acoustic features at different quantization levels, provides critical insights into their relative importance. These insights directly guide the development of our sort-match strategy, which enhances transmission quality in frequency-selective fading channels by allocating channel resources based on data importance.
        Experimental results show that ZeroSC-SR surpasses conventional speech communication systems, demonstrating its effectiveness in real-world applications that demand reliable and efficient speech communication over challenging channels.
      </p>
    </section>

    <figure class="mb-4">
      <img src="Fig/overview.svg" alt="Proposed System Diagram" class="w-full rounded shadow">
      <figcaption class="text-center text-sm text-gray-600 mt-2">Figure 1: Overview of the ZeroSC-SR system structure.</figcaption>
    </figure>

<!-- Proposed System (Revised) -->
<section id="proposed-system" class="mb-12">
  <h2 class="section-title">Proposed System</h2>
  <p class="text-gray-700 mb-4">
    The proposed ZeroSC-SR framework combines semantic (phoneme-level) and acoustic (prompt audio codec) features 
    to enable efficient speech transmission under frequency-selective fading channels 
    <span class="text-gray-500 text-sm">[1, 2]</span>. 
    By integrating phoneme extraction, audio codec compression, and a novel 
    <em>sort-match strategy</em>, ZeroSC-SR mitigates channel impairments while preserving 
    key linguistic and speaker identity information. 
    <strong>Figure 1</strong> shows an overview of the ZeroSC-SR system 
    <span class="text-gray-500 text-sm">[Fig.&nbsp;1 from paper, \cite{defossez2022high,wang2023neural}]</span>.
  </p>

  <!-- 1) Transmitter -->
  <h3 id="transmitter" class="subsection-title">1) Transmitter</h3>
  <p class="text-gray-700 mb-4">
    At the transmitter, the input speech is first split into semantic and acoustic components 
    <span class="text-gray-500 text-sm">[3, 4]</span>. 
    <strong>Phoneme IDs</strong> succinctly capture linguistic content, while 
    <strong>prompt audio codec codes</strong> retain speaker-specific cues 
    via the EnCodec model <span class="text-gray-500 text-sm">[5]</span>. 
    This design adheres to digital transmission principles 
    <span class="text-gray-500 text-sm">[6]</span>, 
    facilitating integration with typical communication blocks such as modulation and 
    error-control coding (e.g., LDPC).
  </p>
  <ul class="list-disc list-inside text-gray-700 mb-4 ml-6">
    <li>
      <strong>Phoneme Extraction:</strong> Converts raw speech into phoneme IDs 
      using an ASR-based pipeline 
      <span class="text-gray-500 text-sm">[\cite{radford2023robust}, \cite{prabhavalkar2024survey}]</span>.
    </li>
    <li>
      <strong>Prompt Selection and Compression:</strong> Selects a short prompt from the 
      input speech and encodes it with EnCodec 
      <span class="text-gray-500 text-sm">[\cite{defossez2022high}]</span>, 
      yielding discrete audio codec codes essential for capturing speaker identity.
    </li>
  </ul>

  <figure class="mb-4">
    <img src="Fig/channel_encoder.svg" alt="Channel Encoder Diagram" class="w-full rounded shadow">
    <figcaption class="text-center text-sm text-gray-600 mt-2">
      <strong>Figure 2:</strong> Channel Encoder Diagram illustrating the segmentation, sorting by importance, 
      and binary conversion of phoneme and codec streams 
      <span class="text-gray-500 text-sm">[\cite{weng2021JSAC}, \cite{9953316}]</span>.
    </figcaption>
  </figure>

  <!-- 2) Sort-Match Strategy for Frequency-Selective Channels -->
  <h3 id="sort-match-strategy" class="subsection-title">
    2) Sort-Match Strategy for Frequency-Selective Channels
  </h3>
  <p class="text-gray-700 mb-4">
    After obtaining modulated data streams, ZeroSC-SR applies a 
    <em>sort-match strategy</em> to address frequency-selective fading 
    <span class="text-gray-500 text-sm">[7, 8]</span>. This method:
  </p>
  <ol class="list-decimal list-inside text-gray-700 mb-4 ml-6">
    <li>
      <strong>Sorts data streams</strong> by importance, prioritizing phoneme IDs and 
      earlier codec stages (shown to be more critical) 
      <span class="text-gray-500 text-sm">[\cite{wang2023neural}, \cite{defossez2022high}]</span>.
    </li>
    <li>
      <strong>Sorts subcarriers</strong> by channel gain magnitude, identifying 
      the most reliable subcarriers 
      <span class="text-gray-500 text-sm">[9]</span>.
    </li>
    <li>
      <strong>Matches critical data</strong> to stronger subcarriers, ensuring 
      higher protection against noise and fading 
      <span class="text-gray-500 text-sm">[\cite{weng2023deep}, \cite{weng2021semantic}]</span>.
    </li>
  </ol>
  <p class="text-gray-700 mb-4">
    This frequency-dependent allocation significantly improves semantic fidelity 
    and speaker similarity under harsh channel conditions, differentiating 
    ZeroSC-SR from prior designs that assume frequency-flat fading 
    <span class="text-gray-500 text-sm">[\cite{9953316}, \cite{weng2021JSAC}, \cite{weng2023deep}]</span>.
  </p>

  <!-- 3) Receiver -->
  <h3 id="receiver" class="subsection-title">3) Receiver</h3>
  <p class="text-gray-700 mb-4">
    On the receiver side, the system reverses the encoding steps and reconstructs 
    the full speech signal 
    <span class="text-gray-500 text-sm">[10]</span>:
  </p>
  <ul class="list-disc list-inside text-gray-700 mb-4 ml-6">
    <li>
      <strong>Channel Decoder:</strong> Demodulates, decodes (LDPC), and reassembles 
      the phoneme IDs and prompt audio codec codes according to importance-based 
      index information 
      <span class="text-gray-500 text-sm">[\cite{shi2021semantic}, \cite{qin2021semantic}]</span>.
    </li>
    <li>
      <strong>Speech Synthesis with VALL-E:</strong> Uses a zero-shot TTS predictor 
      \cite{wang2023neural} to expand the prompt codec codes into a full set of 
      audio codec representations, guided by the transmitted phoneme IDs.
    </li>
    <li>
      <strong>Final Reconstruction:</strong> Employs EnCodec’s decoder 
      \cite{defossez2022high} on the expanded codec sequence to produce the 
      reconstructed speech waveform, preserving both semantic content and speaker 
      characteristics.
    </li>
  </ul>
  <p class="text-gray-700">
    Extensive experiments validate ZeroSC-SR’s advantages over conventional 
    communication systems. The system achieves lower Character Error Rates (CER), 
    higher speaker similarity scores, and maintains a high compression ratio even 
    under noisy or low-power conditions 
    <span class="text-gray-500 text-sm">
      [\cite{9953316}, \cite{weng2021JSAC}, \cite{weng2023deep}, \cite{wang2023neural}]
    </span>.
  </p>

</section>

    <!-- Experiment and Numerical Results -->
    <section id="experiment-results" class="mb-12">
      <h2 class="section-title">Experiment and Numerical Results</h2>
      <p class="text-gray-700 mb-6">
        The performance of the proposed system is evaluated under varying channel conditions, assuming perfect channel state information. Experiments are conducted using the Libri-light dataset on an NVIDIA RTX A4000. Metrics such as Character Error Rate (CER), Word Error Rate (WER), and speaker similarity provide insights into semantic understanding and speaker fidelity. The table below lists key experimental parameters.
      </p>

      <!-- Experiment Parameters Table -->
      <div class="overflow-auto mb-6">
        <table class="min-w-full bg-white border border-gray-300 text-gray-700">
          <thead class="bg-gray-100 border-b">
            <tr>
              <th class="py-2 px-4 text-left">Parameter</th>
              <th class="py-2 px-4 text-left">Value</th>
              <th class="py-2 px-4 text-left">Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr class="border-b">
              <td class="py-2 px-4">Error Control Coding</td>
              <td class="py-2 px-4">LDPC</td>
              <td class="py-2 px-4">---</td>
            </tr>
            <tr class="border-b">
              <td class="py-2 px-4">Modulation Scheme</td>
              <td class="py-2 px-4">4-QAM</td>
              <td class="py-2 px-4">---</td>
            </tr>
            <tr class="border-b">
              <td class="py-2 px-4">Input Speech Length</td>
              <td class="py-2 px-4">~10 seconds</td>
              <td class="py-2 px-4">---</td>
            </tr>
            <tr class="border-b">
              <td class="py-2 px-4">Prompt Length</td>
              <td class="py-2 px-4">2 seconds</td>
              <td class="py-2 px-4">
                <a href="#prompt-length" class="text-blue-600 hover:underline">Sec. IV.A</a>
              </td>
            </tr>
            <tr class="border-b">
              <td class="py-2 px-4">SNR Range</td>
              <td class="py-2 px-4">[-5, 15] dB</td>
              <td class="py-2 px-4">
                <a href="#phoneme-acoustic" class="text-blue-600 hover:underline">Sec. IV.B</a>
              </td>
            </tr>
            <tr class="border-b">
              <td class="py-2 px-4">Stages in Audio Codec Codes</td>
              <td class="py-2 px-4">8</td>
              <td class="py-2 px-4">
                <a href="#importance-analysis" class="text-blue-600 hover:underline">Sec. IV.C</a>
              </td>
            </tr>
          </tbody>
        </table>
      </div>

      <!-- 1) Prompt Length -->
      <h3 id="prompt-length" class="subsection-title">1) Prompt Length</h3>
      <p class="text-gray-700 mb-4">
        We analyze the prompt length in the context of the <em>entire ZeroSC-SR system</em> output (including final reconstruction). Shorter prompts tend to lack sufficient acoustic clues for speaker identity, and can raise error rates. While longer prompts can lower errors, the benefit plateaus after about 2 seconds, balancing system performance with transmission efficiency.
      </p>

      <!-- Audio Samples for Prompt Length -->
      <p class="text-gray-700 mb-4">
        Below are placeholder audio samples from the final system output at different prompt lengths (1s, 2s, and 3s). Notice differences in intelligibility and speaker fidelity.
      </p>
      <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-8">
        <div class="audio-sample">
          <p class="font-medium mb-2">Prompt = 1s</p>
          <p class="text-sm text-gray-600 mb-2">
            Short prompt can omit critical acoustic cues and cause higher WER/CER.
          </p>
          <audio controls class="w-full">
            <source src="audio/Length/1s/121_127105_000008_000000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">Prompt = 2s</p>
          <p class="text-sm text-gray-600 mb-2">
            Balanced prompt length for semantic understanding and speaker features.
          </p>
          <audio controls class="w-full">
            <source src="audio/Length/2s/121_127105_000008_000000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">Prompt = 3s</p>
          <p class="text-sm text-gray-600 mb-2">
            Longer prompt can improve clarity but raises data overhead.
          </p>
          <audio controls class="w-full">
            <source src="audio/Length/3s/121_127105_000008_000000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>

      <!-- 2) Phonemes vs. Acoustic Features -->
      <h3 id="phoneme-acoustic" class="subsection-title">2) Phonemes vs. Acoustic Features</h3>
      <p class="text-gray-700 mb-4">
        ZeroSC-SR simultaneously handles <strong>phoneme transmission</strong> (carrying semantic content) and <strong>acoustic feature transmission</strong> (audio codec codes capturing speaker characteristics). To illustrate their sensitivity to channel noise, we provide separate audio samples at 0 dB, 5 dB, and 10 dB for each transmission type:
      </p>
      <ul class="list-disc list-inside text-gray-700 mb-4 ml-4">
        <li><strong>Phonemes:</strong> Errors in phoneme data severely affect CER/WER.</li>
        <li><strong>Acoustic Features:</strong> Distorted codes lead to loss of speaker identity and prosodic details.</li>
      </ul>

      <!-- Audio Samples for Phoneme Transmission -->
      <p class="font-semibold mt-4 mb-2 text-gray-800">Phoneme Transmission Only</p>
      <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-6">
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 0 dB</p>
          <p class="text-sm text-gray-600 mb-2">
            Extremely noisy, high chance of phoneme errors and semantic loss.
          </p>
          <audio controls class="w-full">
            <source src="audio/Phoneme_Acoustic_SNR/0dB_phoneme/121_127105_000008_000000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 5 dB</p>
          <p class="text-sm text-gray-600 mb-2">
            Moderate noise, phoneme errors still possible but partially mitigated.
          </p>
          <audio controls class="w-full">
            <source src="audio/Phoneme_Acoustic_SNR/5dB_phoneme/121_127105_000008_000000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 10 dB</p>
          <p class="text-sm text-gray-600 mb-2">
            Less noise, higher semantic accuracy in phoneme decoding.
          </p>
          <audio controls class="w-full">
            <source src="audio/Phoneme_Acoustic_SNR/10dB_phoneme/121_127105_000008_000000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>

      <!-- Audio Samples for Acoustic Feature Transmission -->
      <p class="font-semibold mt-4 mb-2 text-gray-800">Acoustic Feature Transmission Only</p>
      <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-8">
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 0 dB</p>
          <p class="text-sm text-gray-600 mb-2">
            Extremely noisy channel for acoustic codes, speaker identity severely distorted.
          </p>
          <audio controls class="w-full">
            <source src="audio/Phoneme_Acoustic_SNR/0dB_acoustic/121_127105_000008_000000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 5 dB</p>
          <p class="text-sm text-gray-600 mb-2">
            Moderate noise, some speaker nuances remain but noticeable artifacts.
          </p>
          <audio controls class="w-full">
            <source src="audio/Phoneme_Acoustic_SNR/5dB_acoustic/121_127105_000008_000000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 10 dB</p>
          <p class="text-sm text-gray-600 mb-2">
            Cleaner channel, closer to original voice quality and prosody.
          </p>
          <audio controls class="w-full">
            <source src="audio/Phoneme_Acoustic_SNR/10dB_acoustic/121_127105_000008_000000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>

      <!-- 3) Importance Analysis of Acoustic Features -->
      <h3 id="importance-analysis" class="subsection-title">3) Importance Analysis of Acoustic Features</h3>
      <p class="text-gray-700 mb-4">
        A deletion-based experiment on the 8-stage audio codec codes reveals certain stages carry more critical information. Removing Stage 1, for example, severely degrades recognition accuracy and speaker similarity, while dropping Stage 5 affects finer details like timbre.
      </p>
      <p class="text-gray-700 mb-4">
        Below are final reconstructed samples when specific stages are removed. Notice how losing early stages impacts semantic clarity <em>and</em> acoustic identity, while dropping later stages primarily influences fine-grained quality.
      </p>
      <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-8">
        <div class="audio-sample">
          <p class="font-medium mb-2">Deleting Stage 1</p>
          <p class="text-sm text-gray-600 mb-2">
            Most crucial data lost, major drop in intelligibility and speaker fidelity.
          </p>
          <audio controls class="w-full">
            <source src="audio/Stage_Delete/stage1/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">Deleting Stage 2</p>
          <p class="text-sm text-gray-600 mb-2">
            Still high-impact data; moderate degradation in clarity.
          </p>
          <audio controls class="w-full">
            <source src="audio/Stage_Delete/stage2/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">Deleting Stage 5</p>
          <p class="text-sm text-gray-600 mb-2">
            Affects finer acoustic cues and timbre, but preserves core semantics.
          </p>
          <audio controls class="w-full">
            <source src="audio/Stage_Delete/stage5/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>

      <!-- 4) Sort-Match Strategy Analysis -->
      <h3 id="sort-match" class="subsection-title">4) Sort-Match Strategy Analysis</h3>
      <p class="text-gray-700 mb-4">
        To address frequency-selective fading, we propose sorting data streams by importance (e.g., phonemes and earlier codec stages first) and matching them to sub-carriers with higher gains. Compared with random allocation, our sort-match strategy consistently yields lower CER/WER and maintains higher speaker similarity, especially under low SNR conditions.
      </p>

      <!-- Audio Samples for Sort-Match Strategy Analysis -->
      <p class="text-gray-700 mb-4">
        Below are system outputs at different SNRs for Sort-Match vs. Random-Match. Pay attention to both semantic clarity and speaker consistency.
      </p>
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
        <!-- SNR = 5 dB -->
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 5 dB: Sort-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Prioritized data on higher-gain subcarriers yields better intelligibility under noise.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/sort/SNR5/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 5 dB: Random-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Data is allocated arbitrarily; expect more errors and degraded speaker fidelity.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/random/SNR5/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>

        <!-- SNR = 10 dB -->
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 10 dB: Sort-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Improved channel conditions preserve both semantic and acoustic cues.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/sort/SNR10/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 10 dB: Random-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Random allocation still introduces artifacts; less consistent speech quality.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/random/SNR10/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>

        <!-- SNR = 15 dB -->
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 15 dB: Sort-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            High SNR plus sorted data yields near-original clarity.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/sort/SNR15/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 15 dB: Random-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Even with good channel quality, random approach lags in fidelity.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/random/SNR15/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>

      <!-- 5) Comparison With Other Models -->
      <h3 id="comparison-models" class="subsection-title">5) Comparison With Other Models</h3>
      <p class="text-gray-700 mb-4">
        We benchmark ZeroSC-SR against EnCodec, DeepSC-S, and LPC. Our system achieves the highest compression ratio while maintaining robust performance in low-power conditions. Though all systems converge at high power levels, ZeroSC-SR excels in challenging environments, retaining both semantic content and speaker identity when channel quality is poor.
      </p>

      <!-- Audio Samples for Comparison With Other Models -->
      <p class="text-gray-700 mb-4">
        Below are placeholder samples comparing ZeroSC-SR, EnCodec, and LPC at different power levels. Observe how ZeroSC-SR preserves clarity at lower power, whereas EnCodec and LPC generally require higher power to achieve comparable performance.
      </p>
      <div class="grid grid-cols-1 md:grid-cols-3 gap-4">
        <!-- ZeroSC-SR (1W, 10W, 100W) -->
        <div class="audio-sample">
          <p class="font-medium mb-2">ZeroSC-SR @ 1W</p>
          <audio controls class="w-full">
            <source src="audio/Model_Compare/ZeroSC-SR/power1/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">ZeroSC-SR @ 10W</p>
          <audio controls class="w-full">
            <source src="audio/Model_Compare/ZeroSC-SR/power10/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">ZeroSC-SR @ 100W</p>
          <audio controls class="w-full">
            <source src="audio/Model_Compare/ZeroSC-SR/power100/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>
      
      <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mt-4">
        <!-- EnCodec (1W, 10W, 100W) -->
        <div class="audio-sample">
          <p class="font-medium mb-2">EnCodec @ 1W</p>
          <audio controls class="w-full">
            <source src="audio/Model_Compare/EnCodec/121_127105_000016_000006_power1.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">EnCodec @ 10W</p>
          <audio controls class="w-full">
            <source src="audio/Model_Compare/EnCodec/121_127105_000016_000006_power10.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">EnCodec @ 100W</p>
          <audio controls class="w-full">
            <source src="audio/Model_Compare/EnCodec/121_127105_000016_000006_power100.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>

      <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mt-4">
        <!-- LPC (10^3 W, 10^4 W, 10^5 W) -->
        <div class="audio-sample">
          <p class="font-medium mb-2">LPC @ 10^3 W</p>
          <audio controls class="w-full">
            <source src="audio/Model_Compare/LPC/8230_279154_000006_000001_power1000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">LPC @ 10^4 W</p>
          <audio controls class="w-full">
            <source src="audio/Model_Compare/LPC/8230_279154_000006_000001_power10000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">LPC @ 10^5 W</p>
          <audio controls class="w-full">
            <source src="audio/Model_Compare/LPC/8230_279154_000006_000001_power10000.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>
    </section>

    <!-- Code -->
    <section id="code" class="mb-12">
      <h2 class="section-title">Code</h2>
      <p class="text-gray-700 mb-4">
        The complete source code for ZeroSC-SR is publicly available. Feel free to clone the repository and follow the instructions in the README to reproduce our experiments and audio demonstrations.
      </p>
      <a href="https://github.com/WinfredCU/ZeroSC-SR.git" class="text-blue-600 hover:underline">
        GitHub Repository
      </a>
    </section>

        <!-- References -->
    <section id="references" class="mb-12">
      <h2 class="section-title">References</h2>
      <ol class="list-decimal list-inside text-gray-700 ml-6">
        <li>
          <strong>[R1]</strong> Weng, Z. &amp; Qin, Z. (2021). "Semantic Communication Systems for Speech Transmission," 
          <em>IEEE Journal on Selected Areas in Communications, 39</em>(8), 2434–2444.
        </li>
        <li>
          <strong>[R2]</strong> Han, T., Yang, Q., Shi, Z., He, S., &amp; Zhang, Z. (2023). 
          "Semantic-Preserved Communication System for Highly Efficient Speech Transmission," 
          <em>IEEE Journal on Selected Areas in Communications, 41</em>(1), 245–259.
        </li>
        <li>
          <strong>[R3]</strong> D{\'e}fossez, A., Copet, J., Synnaeve, G., &amp; Adi, Y. (2022). 
          "High Fidelity Neural Audio Compression." <em>arXiv preprint arXiv:2210.13438</em>.
        </li>
        <li>
          <strong>[R4]</strong> Weng, Z. et al. (2023). 
          "Deep Learning Enabled Semantic Communications with Speech Recognition and Synthesis," 
          <em>IEEE Transactions on Wireless Communications</em>. 
        </li>
        <li>
          <strong>[R5]</strong> Radford, A. et al. (2023). 
          "Robust Speech Recognition via Large-Scale Weak Supervision," 
          <em>International Conference on Machine Learning</em>.
        </li>
        <li>
          <strong>[R6]</strong> Prabhavalkar, R. et al. (2024). 
          "End-to-End Speech Recognition: A Survey," 
          <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32</em>, 325–351.
        </li>
        <li>
          <strong>[R7]</strong> Pfister, H. (2017). "Discrete-Time Signal Processing," 
          <em>Lecture Notes, Duke University</em>.
        </li>
        <li>
          <strong>[R8]</strong> Shi, G. et al. (2021). 
          "From Semantic Communication to Semantic-Aware Networking: Model, Architecture, and Open Problems," 
          <em>IEEE Communications Magazine, 59</em>(8), 44–50.
        </li>
        <li>
          <strong>[R9]</strong> Weng, Z. &amp; Qin, Z. (2021). 
          "Semantic Communications for Speech Signals," 
          <em>ICC 2021-IEEE International Conference on Communications</em>, 1–6.
        </li>
        <li>
          <strong>[R10]</strong> Luo, X. et al. (2022). 
          "Semantic Communications: Overview, Open Issues, and Future Research Directions," 
          <em>IEEE Wireless Communications, 29</em>(1), 210–219.
        </li>
        <li>
          <strong>[R11]</strong> Wang, C. et al. (2023). 
          "Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers," 
          <em>arXiv preprint arXiv:2301.02111</em>.
        </li>
        <li>
          <strong>[R12]</strong> D{\'e}fossez, A. &amp; al. (2022). 
          "High Fidelity Neural Audio Compression," 
          <em>arXiv preprint arXiv:2210.13438</em>. 
          (Note: Example repeated or combined if needed.)
        </li>
      </ol>
    </section>

    <!-- Footer -->
    <footer class="mt-12 text-center text-gray-500 text-sm">
      <p class="mb-2">
        &copy; Wang Zewei
      </p>
    </footer>
  </div>
</body>
</html>
