<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ZeroSC-SR Paper Demo</title>
  <!-- Tailwind CSS -->
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    .audio-sample {
      margin: 10px 0;
    }
    /* Optionally add some custom styling */
    .section-title {
      @apply text-2xl font-bold mb-4;
    }
    .subsection-title {
      @apply text-xl font-bold mb-2;
    }
  </style>
</head>

<body class="bg-gray-50">

  <!-- Container -->
  <div class="container mx-auto px-4 py-8 max-w-4xl">
    
    <!-- Header -->
    <header class="text-center mb-12">
      <h1 class="text-4xl font-bold mb-4">ZeroSC-SR: Leveraging Data Importance for Efficient Semantic Speech Communication</h1>
      <div class="flex flex-wrap justify-center gap-4 mb-6">
        <div class="text-center">
          <p class="font-medium">Zewei Wang</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Guanchong Niu</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Yao Tang</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Man-On Pun</p>
        </div>
        <div class="text-center">
          <p class="font-medium">Tat Ming Lok</p>
        </div>
      </div>
    </header>

    <!-- Abstract -->
    <section class="mb-12">
      <h2 class="section-title">Abstract</h2>
      <p class="text-gray-700">
        Advancements in artificial intelligence have enabled the development of more efficient and semantic-aware speech communication systems. 
        This paper introduces an innovative Zero-Shot Semantic Communication System with Speech Reconstruction (ZeroSC-SR), designed to enhance speech transmission and reconstruction in frequency-selective fading channels.
        By efficiently transmitting phoneme IDs and compressed prompt acoustic features, ZeroSC-SR achieves a high compression ratio and reduces transmitted data size, enabling superior performance under low-power and noisy conditions. 
        We investigate the impact of prompt length on recognition accuracy and system performance, identifying a trade-off between performance and energy efficiency and offering practical guidelines for selecting appropriate prompt lengths.
        Analyzing the intermediate transmission data, which includes phoneme IDs and acoustic features at different quantization levels, provides critical insights into their relative importance. These insights directly guide the development of our sort-match strategy, which enhances transmission quality in frequency-selective fading channels by allocating channel resources based on data importance.
        Experimental results show that ZeroSC-SR surpasses conventional speech communication systems, demonstrating its effectiveness in real-world applications that demand reliable and efficient speech communication over challenging channels.
      </p>
    </section>

    <!-- Introduction -->
    <section class="mb-12">
      <h2 class="section-title">Introduction</h2>
      <p class="text-gray-700 mb-4">
        Traditional speech communication systems rely on methods such as Pulse Code Modulation&nbsp;(PCM) and Linear Predictive Coding&nbsp;(LPC) to represent and transmit audio signals&nbsp;[1][2]. PCM achieves high fidelity by sampling the analog signal at a high rate, resulting in large data sizes&nbsp;[1]. LPC, on the other hand, compresses data by modeling the vocal tract and predicting speech samples, reducing the bit rate but potentially sacrificing some audio quality&nbsp;[3]. While effective for transmitting high-quality audio, these methods focus on precise waveform reproduction without considering the semantic content of speech&nbsp;[4], which can be inefficient when conveying meaning is the primary objective&nbsp;[5][6][7].
      </p>
      <p class="text-gray-700 mb-4">
        Advances in artificial intelligence&nbsp;(AI) have opened new possibilities for improving speech communication systems. Speech, as a primary medium for human interaction, contains rich acoustic and semantic information&nbsp;[8][9][10][11]. AI techniques such as convolutional neural networks&nbsp;(CNNs), bidirectional long short-term memory&nbsp;(BLSTM) networks, and Transformer models&nbsp;[12][13][14] have proven effective in processing and understanding these complex features, enabling novel approaches that focus on transmitting semantic information rather than raw audio signals.
      </p>
      <p class="text-gray-700 mb-4">
        Building on these advancements, the paradigm of semantic communication has emerged, focusing on efficient speech transmission by prioritizing meaning over exact signal reproduction. Unlike traditional methods&nbsp;[1][2][3], the semantic communication system prioritizes the extraction and transmission of information necessary for understanding, employing machine learning&nbsp;(ML) techniques to identify and transmit critical acoustic and semantic features&nbsp;[5][6][7]. By shifting the focus from bit-level accuracy to effectively conveying meaning, these systems reduce the amount of transmitted data while maintaining intelligibility and perceived quality&nbsp;[15][16].
      </p>
      <p class="text-gray-700 mb-4">
        Within semantic speech communication, two primary approaches have been explored: <em>End-to-End Speech Transmission</em> and <em>Text-to-Speech Synthesis</em>. The former encodes features directly from the speech signal to reconstruct the waveform at the receiver&nbsp;[17][18]. For example, the DeepSC-S system&nbsp;[17] utilizes an attention-based encoder-decoder architecture to extract and transmit meaningful features. However, <em>End-to-End Speech Transmission</em> often encounters challenges in preserving semantic accuracy and reliably conveying the intended message under adverse conditions, as evidenced by its relatively high Word Error Rate (WER).
      </p>
      <p class="text-gray-700 mb-4">
        On the other hand, the <em>Text-to-Speech Synthesis</em> approach&nbsp;[19][20] transmits text or text-related features, supplemented by acoustic information, with speech synthesis performed at the receiver. While this approach achieves bandwidth efficiency, it often requires additional speaker identity or acoustic data to reconstruct speaker characteristics at the receiver, limiting its applicability. Moreover, existing works often rely on simplified channel assumptions such as additive white Gaussian noise (AWGN) or frequency-flat fading, making them less robust in practical wide-bandwidth scenarios.
      </p>
      <p class="text-gray-700 mb-4">
        In response to these challenges, this paper proposes a novel semantic speech communication system that effectively transmits semantic and acoustic features under the frequency-selective fading channel. We convert input speech into phoneme IDs to compactly represent semantic content, while a short segment of the original speech is used as a prompt to capture acoustic features via the EnCodec model&nbsp;[21]. We analyze the attributes of the intermediate transmission data and propose a <em>sort-match strategy</em> to enhance transmission efficiency under frequency-selective fading. The channel decoder then recovers phoneme sequences and prompt codec codes, with the VALL-E model&nbsp;[22] predicting extended audio codec codes for final speech reconstruction. 
      </p>
      <p class="text-gray-700 mb-4">
        Extensive experiments validate the feasibility of our ZeroSC-SR system and the effectiveness of the proposed sort-match strategy. Results show that ZeroSC-SR outperforms conventional systems in challenging environments, achieving lower CER and higher speaker similarity scores, particularly under low-power and noisy conditions. Our primary contributions include a zero-shot semantic communication framework, analysis of prompt lengths, data importance evaluation, and robust integration with digital communication systems.     
      </p>
    </section>

    <!-- Proposed System -->
    <section class="mb-12">
      <h2 class="section-title">Proposed System</h2>
      <p class="text-gray-700 mb-4">
        The ZeroSC-SR system introduces a cutting-edge approach to semantic speech communication, addressing challenges in transmitting speech over frequency-selective fading channels. By combining phoneme-based semantic transmission with advanced audio codec compression, the system achieves efficient data transmission and robust performance under adverse conditions.
      </p>

      <!-- System Model Structure -->
      <h3 class="subsection-title">System Model Structure</h3>
      <ul class="list-disc list-inside text-gray-700 mb-4">
        <li>
          At the <strong>transmitter</strong>, input speech is converted into phoneme IDs and compressed acoustic features using audio codec codes from the EnCodec model.
        </li>
        <li>
          During <strong>channel transmission</strong>, a novel sort-match strategy prioritizes data by importance, optimizing resource allocation under frequency-selective fading conditions.
        </li>
        <li>
          At the <strong>receiver</strong>, phoneme sequences and codec codes are recovered, and the VALL-E model predicts extended codec codes to reconstruct speech using the EnCodec model.
        </li>
      </ul>

      <!-- Contributions -->
      <h3 class="subsection-title">Main Contributions</h3>
      <ul class="list-disc list-inside text-gray-700 mb-4">
        <li>
          <strong>Innovative Zero-Shot Semantic Communication System:</strong> Enables efficient digital transmission of both phonemes and acoustic features over frequency-selective fading channels, achieving a high compression ratio and superior performance.
        </li>
        <li>
          <strong>Trade-off Analysis Between Prompt Length and Performance:</strong> Evaluates the influence of prompt length on recognition accuracy and system performance, identifying a practical trade-off between energy efficiency and performance.
        </li>
        <li>
          <strong>Impact Analysis of Intermediate Transmission Data:</strong> Investigates the role of phoneme information and audio codec codes at various quantization levels, guiding the sort-match strategy design.
        </li>
        <li>
          <strong>Novel Sort-Match Strategy:</strong> Allocates subcarriers by ranking data importance to enhance performance in frequency-selective fading channels, outperforming prior frequency-flat fading assumptions.
        </li>
        <li>
          <strong>Efficient Integration with Digital Communication:</strong> Replaces traditional mel spectrograms with audio codec codes for intermediate representation, reducing transmitted data size. Incorporates VALL-E for zero-shot synthesis, enabling speech reconstruction for unseen speakers.
        </li>
      </ul>
      
      <figure class="mb-4">
        <img src="Fig/overview.svg" alt="Proposed System Diagram" class="w-full rounded shadow">
        <figcaption class="text-center text-sm text-gray-600 mt-2">Figure: Overview of the ZeroSC-SR system structure.</figcaption>
      </figure>
    </section>


    <!-- Experiment and Numerical Results -->
    <section class="mb-12">
      <h2 class="section-title">Experiment and Numerical Results</h2>
      <p class="text-gray-700 mb-6">
        The performance of the proposed system is evaluated under varying channel conditions, assuming perfect channel state information. Experiments are conducted using the Libri-light dataset&nbsp;[23] on an NVIDIA RTX A4000. The evaluation metrics, including the CER, WER&nbsp;[24], and speaker similarity&nbsp;[25], provide insights into the system’s semantic understanding and speaker fidelity. Table&nbsp;1 (below) lists the key experimental parameters.
      </p>

      <!-- Experiment Parameters Table (Optional Example) -->
      <div class="overflow-auto mb-6">
        <table class="min-w-full bg-white border border-gray-300 text-gray-700">
          <thead class="bg-gray-100 border-b">
            <tr>
              <th class="py-2 px-4 text-left">Parameter</th>
              <th class="py-2 px-4 text-left">Value</th>
              <th class="py-2 px-4 text-left">Relevant Sections</th>
            </tr>
          </thead>
          <tbody>
            <tr class="border-b">
              <td class="py-2 px-4">Error Control Coding</td>
              <td class="py-2 px-4">LDPC</td>
              <td class="py-2 px-4">–</td>
            </tr>
            <tr class="border-b">
              <td class="py-2 px-4">Modulation Scheme</td>
              <td class="py-2 px-4">4-QAM</td>
              <td class="py-2 px-4">–</td>
            </tr>
            <tr class="border-b">
              <td class="py-2 px-4">Input Speech Length</td>
              <td class="py-2 px-4">~10 seconds</td>
              <td class="py-2 px-4">–</td>
            </tr>
            <tr class="border-b">
              <td class="py-2 px-4">Prompt Length</td>
              <td class="py-2 px-4">2 seconds</td>
              <td class="py-2 px-4">Sec. IV.A</td>
            </tr>
            <tr class="border-b">
              <td class="py-2 px-4">SNR Range</td>
              <td class="py-2 px-4">[-5, 15] dB</td>
              <td class="py-2 px-4">Sec. IV.B</td>
            </tr>
            <tr class="border-b">
              <td class="py-2 px-4">Stages in Audio Codec Codes</td>
              <td class="py-2 px-4">8</td>
              <td class="py-2 px-4">Sec. IV.C</td>
            </tr>
          </tbody>
        </table>
      </div>

      <!-- 1) Prompt Length -->
      <h3 class="subsection-title">1) Prompt Length</h3>
      <p class="text-gray-700 mb-4">
        The length of the transmitted prompt impacts both system performance and the required transmission resources. We aim to determine an appropriate prompt length that reduces resource usage while maintaining satisfactory performance. 
        Experimental results show that increasing the prompt length reduces recognition errors (CER/WER), stabilizing around 2 seconds. After 2 seconds, the decrease in error rates slows, indicating diminishing returns. From a practical standpoint, 2 seconds of prompt length is a sweet spot for balancing performance and transmission efficiency.
      </p>

      <!-- 2) Phonemes vs. Acoustic Features -->
      <h3 class="subsection-title">2) Phonemes vs. Acoustic Features</h3>
      <p class="text-gray-700 mb-4">
        We further investigate the sensitivity of phoneme transmission and acoustic feature transmission to channel noise. Phoneme accuracy significantly affects CER and WER, whereas acoustic features primarily influence speaker similarity. Our findings highlight that ensuring high SNR for phoneme data is crucial for comprehension, while acoustic features require relatively good SNR to maintain speaker fidelity.
      </p>

      <!-- 3) Importance Analysis of Acoustic Features -->
      <h3 class="subsection-title">3) Importance Analysis of Acoustic Features</h3>
      <p class="text-gray-700 mb-4">
        We performed a deletion-based experiment on the 8-stage audio codec codes. Removing Stage 1 caused the largest drop in performance across WER, CER, and speaker similarity. Stages 2 and 3 remained influential, while later stages contributed primarily to speaker fidelity. This indicates a descending order of importance from Stage 1 to Stage 8.
      </p>

      <!-- 4) Sort-Match Strategy Analysis -->
      <h3 class="subsection-title">4) Sort-Match Strategy Analysis</h3>
      <p class="text-gray-700 mb-4">
        To address frequency-selective fading, we propose sorting data streams by importance (phonemes as highest priority, followed by codec stages) and matching them to sub-carriers with corresponding gains. Compared with a random allocation, our sort-match strategy yields significantly lower WER and CER across all tested SNRs, maintaining high speaker similarity even at low SNR.
      </p>

      <!-- 5) Comparison With Other Models -->
      <h3 class="subsection-title">5) Comparison With Other Models</h3>
      <p class="text-gray-700 mb-4">
        We benchmark ZeroSC-SR against EnCodec&nbsp;[21], DeepSC-S&nbsp;[17], and LPC&nbsp;[3]. Our system achieves the highest compression ratio (nearly 387 for 10 seconds of speech) and maintains robust performance at low power levels. Although performance converges at high power levels, ZeroSC-SR proves superior in challenging conditions, balancing semantic understanding and speaker similarity with minimal data transmission.
      </p>
    </section>

    <!-- Audio Samples -->
    <section class="mb-12">
      <h2 class="section-title">Audio Samples</h2>
      <p class="text-gray-700 mb-4">
        Below are audio examples demonstrating the effect of different channel strategies (Sort-Match vs. Random-Match) and varying SNR values. We encourage you to listen for differences in speech intelligibility and speaker fidelity.
      </p>
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
        <!-- SNR = 5 dB -->
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 5 dB: Sort-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Here, critical data (phonemes, early codec stages) are matched to better subcarriers. Listen for improved intelligibility in low-SNR conditions.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/sort/SNR5/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 5 dB: Random-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Data streams are randomly assigned to subcarriers. Notice higher distortion and reduced speaker clarity.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/random/SNR5/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>

        <!-- SNR = 10 dB -->
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 10 dB: Sort-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Moderate channel quality allows better fidelity, especially when data importance is prioritized.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/sort/SNR10/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 10 dB: Random-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Random assignment can still produce audible artifacts and occasional errors.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/random/SNR10/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>

        <!-- SNR = 15 dB -->
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 15 dB: Sort-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Higher SNR highlights the sort-match strategy's ability to preserve speaker characteristics.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/sort/SNR15/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
        <div class="audio-sample">
          <p class="font-medium mb-2">SNR = 15 dB: Random-Match Strategy</p>
          <p class="text-sm text-gray-600 mb-2">
            Although channel quality improves, random allocation still falls behind in clarity compared to sort-match.
          </p>
          <audio controls class="w-full">
            <source src="audio/Sort_Match/random/SNR15/121_127105_000016_000006.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
        </div>
      </div>
    </section>

    <!-- Code -->
    <section class="mb-12">
      <h2 class="section-title">Code</h2>
      <p class="text-gray-700 mb-4">
        The complete source code for ZeroSC-SR is publicly available. Feel free to clone the repository and follow the instructions in the README to reproduce our experiments and audio demonstrations.
      </p>
      <a href="https://github.com/WinfredCU/ZeroSC-SR.git" class="text-blue-600 hover:underline">
        GitHub Repository
      </a>
    </section>

    <!-- Footer (Optional) -->
    <footer class="mt-12 text-center text-gray-500 text-sm">
      <p class="mb-2">
        For any questions or feedback, please contact the authors.
      </p>
      <p>
        &copy; 2024 ZeroSC-SR Team
      </p>
    </footer>
  </div>
</body>
</html>
